{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve NetCDF and model gridded climate time-series for a watershed\n",
    "\n",
    "### Case study:  the Sauk-Suiattle Watershed\n",
    "<img src=\"http://www.sauk-suiattle.com/images/Elliott.jpg\" \n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "### Use this Jupyter Notebook to:\n",
    "    1. Prepare computing environment\n",
    "    2. Get list of grid cells\n",
    "    3. NetCDF retrieval and clipping to a spatial extent\n",
    "    4. Extract NetCDF metadata and convert NetCDFs to 1D ASCII time-series files\n",
    "    5. Visualize the average monthly total precipitations\n",
    "    6. Apply summary values as modeling inputs\n",
    "    7. Visualize modeling outputs\n",
    "    8. Save results in a new HydroShare resource\n",
    "\n",
    "##### For inquiries about functions, please refer to https://github.com/freshwater-initiative/Observatory \n",
    "\n",
    "<br/><br/><br/>\n",
    "<img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\"\n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "<br/><br/>\n",
    "### Special thanks to Nicoleta Cristea and Jeff Keck for their contribution to this work\n",
    "#### This data is compiled to digitally observe the watersheds, powered by HydroShare. <br/>Provided by the Watershed Dynamics Group, Dept. of Civil and Environmental Engineering, University of Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Prepare computing environment\n",
    "\n",
    "Here, we import ogh and several requisite libraries. Then, map directories that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# If OGH is not installed, uncomment and run the line below\n",
    "\n",
    "#!conda install -c conda-forge ogh xarray=0.11.2 --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silencing warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling input params\n",
    "notebookdir = os.getcwd()\n",
    "InputFile = os.path.join(os.getcwd(),'ecohyd_inputs.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a secure connection with HydroShare by instantiating the hydroshare class that is defined within hs_utils. In addition to connecting with HydroShare, this command also sets and prints environment variables for several parameters that will be useful for saving work back to HydroShare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utilities import hydroshare\n",
    "hs=hydroshare.hydroshare()\n",
    "homedir = os.getcwd()\n",
    "os.chdir(homedir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis libraries\n",
    "import ogh\n",
    "from ogh import oxl # xarray 0.11.2\n",
    "from ecohydrology_model_functions import run_ecohydrology_model, plot_results\n",
    "from landlab import imshow_grid, CLOSED_BOUNDARY\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# initialize ogh metadata\n",
    "meta_file = dict(ogh.ogh_meta())\n",
    "sorted(meta_file.keys())\n",
    "\n",
    "# initialize list of outputs\n",
    "files=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get list of grid cells\n",
    "\n",
    "For visualization purposes, we will also remap the study site shapefile, which is stored in HydroShare at the following url: https://www.hydroshare.org/resource/c532e0578e974201a0bc40a37ef2d284/. Since the shapefile was previously migrated, we can select 'N' for no overwriting.\n",
    "\n",
    "In the usecase1 notebook, the treatgeoself function identified the gridded cell centroid coordinates that overlap with our study site. These coordinates were documented within the mapping file, which will be remapped here. In the usecase2 notebook, the downloaded files were cataloged within the mapping file, so we will use the mappingfileSummary function to characterize the files available for Sauk-Suiattle for each gridded data product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1/16-degree Gridded cell centroids\n",
    "\"\"\"\n",
    "# List of available data\n",
    "hs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')\n",
    "NAmer = hs.content['NAmer_dem_list.shp']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sauk\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\n",
    "sauk = hs.content['wbdhub12_17110006_WGS84_Basin.shp']\n",
    "\n",
    "# reproject the shapefile into WGS84\n",
    "ogh.reprojShapefile(sourcepath=sauk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the target grid cells within a watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# map the mappingfiles from usecase1\n",
    "mappingfile1=ogh.treatgeoself(shapefile=sauk, NAmer=NAmer, buffer_distance=0.06,\n",
    "                              mappingfile=os.path.join(homedir,'Sauk_mappingfile.csv'))\n",
    "\n",
    "files.append(mappingfile1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  NetCDF retrieval and clipping to a spatial extent\n",
    "\n",
    "This section performs computations and generates plots of the Livneh 2013 and Salathe 2014 mean temperature and mean total monthly precipitation in order to compare them with each other. The generated plots are automatically downloaded and saved as .png files within the \"homedir\" directory.\n",
    "\n",
    "Let's compare the Livneh 2013 and Salathe 2014 using the period of overlapping history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(oxl.get_x_dailymet_Livneh2013_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function get_x_dailymet_livneh2013_raw retrieves and clips NetCDF files archived within the University of Colorado Boulder repository. This archive contains daily data from January 1970 through December 1979 (10 years). Each netcdf file is comprised of meteorologic and VIC hydrologic outputs for a calendar month. The expected number of files for 1970 would be 12 files (12 months for 1 year). \n",
    "\n",
    "In the code chunk below, 40 parallel workers will be initialized to distribute file retrieval and spatial clipping tasks. For each worker, they will get the requested file, clip the netcdf file to gridded cell centroids within the the provided bounding box, then return the location of the spatially clipped output files.\n",
    "\n",
    "Provide the home and subdirectory where the cropped NetCDF files will be stored. Also provide the spatial bounds (in WGS84) to crop the NetCDF files upon download. Finally, provide the number of workers to carry out the download tasks, and the start and end date of the files of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "maptable, nstations = ogh.mappingfileToDF(mappingfile1, summary=True)\n",
    "spatialbounds = {'minx':maptable.LONG_.min(), 'maxx':maptable.LONG_.max(),\n",
    "                 'miny':maptable.LAT.min(), 'maxy':maptable.LAT.max()}\n",
    "\n",
    "outputfiles = oxl.get_x_dailymet_Livneh2013_raw(homedir=homedir,\n",
    "                                                subdir='livneh2013/Daily_MET_1970_1970/raw_netcdf',\n",
    "                                                spatialbounds=spatialbounds,\n",
    "                                                nworkers=6,\n",
    "                                                start_date='1970-01-01', end_date='1970-12-31')\n",
    "\n",
    "files.extend(outputfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract NetCDF metadata and convert NetCDFs to 1D ASCII time-series files\n",
    "\n",
    "Provide the home and subdirectory where the ASCII files will be stored, the source_directory of netCDF files, and the mapping file to which the resulting ASCII files will be cataloged. Also, provide the Pandas Datetime code for the frequency of the time steps. Finally, provide the catalog label that will be used for the mapping file catalog and the metadata file label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# convert the netCDF files into daily ascii time-series files for each gridded location\n",
    "outfilelist = oxl.netcdf_to_ascii(homedir=homedir, \n",
    "                                  subdir='livneh2013/Daily_MET_1970_1970/raw_ascii', \n",
    "                                  source_directory=os.path.join(homedir, 'livneh2013/Daily_MET_1970_1970/raw_netcdf'),\n",
    "                                  mappingfile=mappingfile1,\n",
    "                                  temporal_resolution='D',\n",
    "                                  meta_file=meta_file,\n",
    "                                  catalog_label='sp_dailymet_livneh_1970_1970')\n",
    "\n",
    "# files.extend(outfilelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(meta_file.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of climate variables for the long-term mean (ltm).\n",
    "#### INPUT: gridded meteorology ASCII files located from the Sauk-Suiattle Mapping file. The inputs to gridclim_dict() include the folder location and name of the hydrometeorology data, the file start and end, the analysis start and end, and the elevation band to be included in the analsyis (max and min elevation). <br/>OUTPUT: dictionary of dataframes where rows are temporal summaries and columns are spatial summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_file['sp_dailymet_livneh_1970_1970']['variable_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the metadata\n",
    "metafile_path = os.path.join(homedir, 'test.json')\n",
    "ogh.saveDictOfDf(dictionaryObject=meta_file, outfilepath=metafile_path)\n",
    "files.append(metafile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ltm = ogh.gridclim_dict(mappingfile=mappingfile1,\n",
    "                        metadata=meta_file,\n",
    "                        dataset='sp_dailymet_livneh_1970_1970',\n",
    "                        variable_list=['Prec','Tmax','Tmin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(ltm.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the total monthly and yearly precipitation, as well as the mean values across time and across stations\n",
    "#### INPUT: daily precipitation for each station from the long-term mean dictionary (ltm) <br/>OUTPUT: Append the computed dataframes and values into the ltm dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract metadata\n",
    "dr = meta_file['sp_dailymet_livneh_1970_1970']\n",
    "\n",
    "# compute sums and mean monthly an yearly sums\n",
    "ltm = ogh.aggregate_space_time_sum(df_dict=ltm,\n",
    "                                   suffix='Prec_sp_dailymet_livneh_1970_1970',\n",
    "                                   start_date=dr['start_date'],\n",
    "                                   end_date=dr['end_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the name of the analytical dataframes and values within ltm\n",
    "sorted(ltm.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the destination path for the dictionary of dataframes\n",
    "ltm_sauk=os.path.join(homedir, 'ltm_1970_1970_sauk.json')\n",
    "ogh.saveDictOfDf(dictionaryObject=ltm, outfilepath=ltm_sauk)\n",
    "files.append(ltm_sauk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize the average monthly total precipitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two lowest elevation locations\n",
    "lowE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=164)\n",
    "\n",
    "# one highest elevation location\n",
    "highE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=2216)\n",
    "\n",
    "# combine references together\n",
    "reference_lines = highE_ref + lowE_ref\n",
    "reference_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: dataframe with each month as a row and each station as a column. \n",
    "# OUTPUT: A png file that represents the distribution across stations (in Wateryear order)\n",
    "\n",
    "ogh.renderValueInBoxplot(ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'],\n",
    "                         outfilepath=os.path.join(homedir, 'totalMonthlyRainfall.png'), \n",
    "                         plottitle='Total monthly rainfall',\n",
    "                         time_steps='month',\n",
    "                         wateryear=True,\n",
    "                         reference_lines=reference_lines,\n",
    "                         ref_legend=True,\n",
    "                         value_name='Total daily precipitation (mm)',\n",
    "                         cmap='seismic_r',\n",
    "                         figsize=(6,6))\n",
    "\n",
    "files.append(os.path.join(homedir, 'totalMonthlyRainfall.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# INPUT: dataframe with each month as a row and each station as a column. \n",
    "# OUTPUT: A png file that represents the spatial distribution for a select month (December)\n",
    "ogh.renderValuesInPoints(ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'], \n",
    "                         vardf_dateindex=12, \n",
    "                         shapefile=sauk, \n",
    "                         cmap='seismic_r',\n",
    "                         outfilepath='test.png', \n",
    "                         plottitle='December total rainfall',\n",
    "                         colorbar_label='Total monthly rainfall (mm)', \n",
    "                         figsize=(1.5,1.5))\n",
    "\n",
    "files.append(os.path.join(homedir, 'test.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply summary values as modeling inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# compute the dimensions of the raster\n",
    "minx2, miny2, maxx2, maxy2 = oxl.calculateUTMbounds(mappingfile=mappingfile1,\n",
    "                                                    mappingfile_crs={'init':'epsg:4326'},\n",
    "                                                    spatial_resolution=0.06250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(minx2, miny2, maxx2, maxy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate a raster\n",
    "\n",
    "The Ecohydrology vegetation model within Landlab is run for a 2D landscape. The data input should be a raster array object configured for the amount of grid cell dimensions desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(oxl.rasterDimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a raster with 1kmx1km grid cells\n",
    "raster, row_list, col_list = oxl.rasterDimensions(minx=minx2, miny=miny2, maxx=maxx2, maxy=maxy2, dx=1000, dy=1000)\n",
    "raster.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher resolution children of gridded cells \n",
    "### get data from Lower resolution parent grid cells to the children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(oxl.mappingfileToRaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# landlab raster node crossmap to gridded cell id\n",
    "nodeXmap, raster, m = oxl.mappingfileToRaster(mappingfile=mappingfile1, \n",
    "                                              spatial_resolution=0.06250,\n",
    "                                              minx=minx2, miny=miny2, maxx=maxx2, maxy=maxy2,\n",
    "                                              dx=1000, dy=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the raster dimensions\n",
    "raster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nodeXmap.plot(column='ELEV', figsize=(10,10), legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate vector array of December monthly precipitation\n",
    "prec_vector = oxl.rasterVector(vardf=ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'],\n",
    "                               vardf_dateindex=12,\n",
    "                               crossmap=nodeXmap,\n",
    "                               nodata=-9999)\n",
    "\n",
    "# close-off areas without data\n",
    "raster.status_at_node[prec_vector==-9999] = CLOSED_BOUNDARY\n",
    "\n",
    "fig =plt.figure(figsize=(10,10))\n",
    "imshow_grid(raster, \n",
    "            prec_vector,\n",
    "            var_name='Monthly precipitation',\n",
    "            var_units=meta_file['sp_dailymet_livneh_1970_1970']['variable_info']['Prec'].attrs['units'],\n",
    "            color_for_closed='black', \n",
    "            cmap='seismic_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_vector = oxl.rasterVector(vardf=ltm['meanbymonth_Tmax_sp_dailymet_livneh_1970_1970'],\n",
    "                               vardf_dateindex=12,\n",
    "                               crossmap=nodeXmap,\n",
    "                               nodata=-9999)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "imshow_grid(raster, \n",
    "            tmax_vector,\n",
    "            var_name='Daily maximum temperature',\n",
    "            var_units=meta_file['sp_dailymet_livneh_1970_1970']['variable_info']['Tmax'].attrs['units'],\n",
    "            color_for_closed='black', symmetric_cbar=False, cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin_vector = oxl.rasterVector(vardf=ltm['meanbymonth_Tmin_sp_dailymet_livneh_1970_1970'],\n",
    "                               vardf_dateindex=12,\n",
    "                               crossmap=nodeXmap,\n",
    "                               nodata=-9999)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "imshow_grid(raster, \n",
    "            tmin_vector,\n",
    "            var_name='Daily minimum temperature',\n",
    "            var_units=meta_file['sp_dailymet_livneh_1970_1970']['variable_info']['Tmin'].attrs['units'],\n",
    "            color_for_closed='black', symmetric_cbar=False, cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a raster vector back to geospatial presentation\n",
    "t2, t3 = oxl.rasterVectorToWGS(prec_vector, nodeXmap=nodeXmap, UTM_transformer=m)\n",
    "\n",
    "t2.crs = {'init':'epsg:4326'}\n",
    "t2.drop('raster_geom', axis=1).to_file(os.path.join(homedir,'hires_sauk.shp'))\n",
    "t2.plot(column='value', figsize=(10,6), legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert to projection for North Washington to correct latitude stretch\n",
    "t2 = t2.to_crs({'init':'epsg:3857'})\n",
    "t2.plot(column='value', figsize=(10,6), legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in the shp file parts\n",
    "files.extend([os.path.join(homedir, shpfile) for shpfile in os.listdir(homedir) if 'hires_sauk' in shpfile])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the meteorological summary for 1970 as the model input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the input vector with 15000 repetitions \n",
    "inputvectors = {'precip_met': np.tile(ltm['meandaily_Prec_sp_dailymet_livneh_1970_1970'], 15000),\n",
    "                'Tmax_met': np.tile(ltm['meandaily_Tmax_sp_dailymet_livneh_1970_1970'], 15000),\n",
    "                'Tmin_met': np.tile(ltm['meandaily_Tmin_sp_dailymet_livneh_1970_1970'], 15000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# run ecohydrology model for 100000 storms\n",
    "\n",
    "(VegType_low, yrs_low, debug_low) = run_ecohydrology_model(raster,\n",
    "                                                           input_data=inputvectors,\n",
    "                                                           input_file=InputFile,\n",
    "                                                           synthetic_storms=False,\n",
    "                                                           number_of_storms=100000,\n",
    "                                                           pet_method='PriestleyTaylor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_results(raster, VegType_low, yrs_low, yr_step=yrs_low-1)\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(homedir,'grid_low.png'))\n",
    "files.append(os.path.join(homedir,'grid_low.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "imshow_grid(raster,\n",
    "            prec_vector,\n",
    "            var_name='Monthly precipitation',\n",
    "            var_units=meta_file['sp_dailymet_livneh_1970_1970']['variable_info']['Prec'].attrs['units'],\n",
    "            color_for_closed='black', \n",
    "            cmap='seismic_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the \"average monthly total precipitation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the results back into HydroShare\n",
    "<a name=\"creation\"></a>\n",
    "\n",
    "Using the `hs_utils` library, the results of the Geoprocessing steps above can be saved back into HydroShare.  First, define all of the required metadata for resource creation, i.e. *title*, *abstract*, *keywords*, *content files*.  In addition, we must define the type of resource that will be created, in this case *genericresource*.  \n",
    "\n",
    "***Note:*** Make sure you save the notebook at this point, so that all notebook changes will be saved into the new HydroShare resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total files and image to migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file downloaded onto the server folder, move to a new HydroShare Composite Resource\n",
    "title = 'Computed spatial-temporal summaries for Sauk-Suiattle for 1970'\n",
    "abstract = 'This resource contains the computed summaries for the Meteorology data from Livneh et al. 2013.'\n",
    "keywords = ['Sauk-Suiattle', 'Livneh 2013', 'climate', 'hydromet', 'watershed', 'visualizations and summaries'] \n",
    "rtype = 'compositeresource'\n",
    "parent_resource_id = homedir.replace('/data/contents','').rsplit('/',1)[1]\n",
    "\n",
    "# create the new resource\n",
    "resource_id = hs.createHydroShareResource(abstract, \n",
    "                                          title,\n",
    "                                          keywords=keywords, \n",
    "                                          resource_type=rtype, \n",
    "                                          content_files=files,\n",
    "                                          derivedFromId=parent_resource_id,\n",
    "                                          public=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
